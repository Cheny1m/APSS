{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "from test import get_partiton_cost_sequence, pipe_ast,pi2partition\n",
    "import numpy as np\n",
    "from nets.attention_model import set_decode_type\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pprint as pp\n",
    "\n",
    "# import torch.multiprocessing as mp\n",
    "# import torch\n",
    "# import torch.optim as optim\n",
    "import mindspore\n",
    "from mindspore import ops\n",
    "from tensorboard_logger import Logger as TbLogger\n",
    "\n",
    "from nets.critic_network import CriticNetwork\n",
    "from options import get_options\n",
    "from train1 import train_epoch, validate, get_inner_model\n",
    "from reinforce_baselines_pp import  RolloutBaselinePP,WarmupBaseline\n",
    "from reinforce_baselines import NoBaseline, ExponentialBaseline, CriticBaseline, RolloutBaseline#, WarmupBaseline\n",
    "from nets.attention_model import AttentionModel\n",
    "from nets.pointer_network import PointerNetwork, CriticNetworkLSTM\n",
    "from utils import load_problem, load_model,load_model_temp ##torch_load_cpu\n",
    "from test import test\n",
    "from problems.pp.problem_pp import get_pp_costs\n",
    "import math\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import shutil\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# import torch\n",
    "# from torch import optim as optim\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import mindspore.nn as nn\n",
    "import sys\n",
    "sys.path.append(\"/root/cym/AMP/src/\")\n",
    "from sa import amp_no_placement_strategy\n",
    "from cost_het_model import   get_cost_e,dp_cost,get_cost_c\n",
    "from amp_utils import simulate, to_float_torch\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import json\n",
    "import copy\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from amp_utils import rank2axis, axis2rank, get_host\n",
    "from pipe import pipe_ds, pipe_ast, pipe_cost, pipe_uniform, pipe_gpt2\n",
    "from utils import load_problem #torch_load_cpu\n",
    "# number of GPU per node, number of nodes\n",
    "M = 4\n",
    "N = 4\n",
    "# home_path = \"/home/oj/distributed_floder/research/AMP\" #os.environ['HOME']\n",
    "home_path = \"/root/cym/AMP\"\n",
    "dir_path = os.path.join(home_path, 'amp_main_logs')\n",
    "if not os.path.exists(dir_path):\n",
    "    os.mkdir(dir_path)\n",
    "\n",
    "cluster_info = {}\n",
    "cast = ops.Cast()\n",
    "for i in range(N):\n",
    "    # cluster_info[i] = [torch.tensor([10 * 1e9 / 32]).float(), torch.tensor([170 * 1e9 / 32]).float()]\n",
    "    cluster_info[i] = [cast(mindspore.Tensor([10 * 1e9 / 32]),mindspore.float32), cast(mindspore.Tensor([170 * 1e9 / 32]),mindspore.float32)]\n",
    "# print(cluster_info)\n",
    "depth = [12,0,0,0,0,12]\n",
    "model_config = {\"hidden_size\": cast(mindspore.Tensor([1024]),mindspore.float32),\n",
    "    \"sequence_length\": cast(mindspore.Tensor([1024]),mindspore.float32),\n",
    "    \"num_layers\": cast(mindspore.Tensor([sum(depth)]),mindspore.float32),\n",
    "    \"vocab_size\":cast(mindspore.Tensor([52256]),mindspore.float32),\n",
    "    \"type\":\"transgan\",\n",
    "    \"depth\": depth,\n",
    "    \"bottom\":9}\n",
    "\n",
    "config_h = int((model_config[\"hidden_size\"]).item())\n",
    "config_n = int(model_config[\"num_layers\"].item())\n",
    "time_stamp = int(time.time())\n",
    "exp_name = f\"het_model\"\n",
    "record_file = f\"{os.path.join(dir_path, exp_name)}_{time_stamp}.txt\"\n",
    "simulate_dir = os.path.join(home_path, \"amp_simulate\")\n",
    "if not os.path.exists(simulate_dir):\n",
    "    os.mkdir(simulate_dir)\n",
    "\n",
    "# remove cache directory from last run\n",
    "if os.path.exists(os.path.join(home_path, \"tmp\")):\n",
    "    for root, dirs, files in os.walk(os.path.join(home_path, \"tmp\")):\n",
    "        for f in files:\n",
    "            os.unlink(os.path.join(root, f))\n",
    "\n",
    "# save this name to env\n",
    "os.environ[\"amp_log_path\"] = record_file\n",
    "def load_all_model():\n",
    "    models={}\n",
    "    # models[2], _ = load_model(\"./outputs1/pp_8/pp_8_4_20231017T154608/epoch-14.ckpt\")\n",
    "    # models[4],_ =  load_model(\"./outputs1/pp_8/pp_8_4_20231017T154608/epoch-14.ckpt\")\n",
    "    # models[8],_ = load_model(\"./outputs1/pp_8/pp_8_4_20231017T154608/epoch-14.ckpt\")\n",
    "    # models[16],_=  load_model(\"./outputs1/pp_8/pp_8_4_20231017T154608/epoch-14.ckpt\")\n",
    "    models[2], _ = load_model_temp(\"./outputs1/pp_8/pp_8_4_20231017T154608/epoch-14.ckpt\",1)\n",
    "    models[4],_ =  load_model_temp(\"./outputs1/pp_8/pp_8_4_20231017T154608/epoch-14.ckpt\",3)\n",
    "    models[8],_ = load_model_temp(\"./outputs1/pp_8/pp_8_4_20231017T154608/epoch-14.ckpt\",7)\n",
    "    models[16],_=  load_model_temp(\"./outputs1/pp_8/pp_8_4_20231017T154608/epoch-14.ckpt\",15)\n",
    "    models[2] = models[2].set_train(False)\n",
    "    models[4] = models[4].set_train(False)\n",
    "    models[8] = models[8].set_train(False)\n",
    "    models[16] = models[16].set_train(False)\n",
    "    # models[2] = models[2].eval()\n",
    "    # models[2] = models[2].cuda()\n",
    "    # models[4] = models[4].eval()\n",
    "    # models[4] = models[4].cuda()\n",
    "    # models[8] = models[8].eval()\n",
    "    # models[8] = models[8].cuda()\n",
    "    # models[16] = models[16].eval()\n",
    "    # models[16] = models[16].cuda()\n",
    "    \n",
    "    return models\n",
    "def pi2partition(pi,node_size):\n",
    "    pi.sort()\n",
    "    # print(pi)\n",
    "    assert node_size > pi[-1]+1, print(node_size,pi)\n",
    "    piadd1 = [i+1 for i in pi]\n",
    "    piadd1 = [0] + piadd1 + [node_size]\n",
    "    partition = []\n",
    "    for i, p in enumerate(piadd1):\n",
    "        if i ==0:\n",
    "            continue\n",
    "        partition.append(p - piadd1[i-1])\n",
    "    return partition\n",
    "def get_partiton_cost_sequence(data,cost_c_data,partition):\n",
    "    # data = torch.Tensor(data)\n",
    "    pp=len(partition)\n",
    "    s = partition\n",
    "    p = [s[0]-1]\n",
    "\n",
    "    for i in range(1, pp):\n",
    "        p.append(p[i-1] + s[i])\n",
    "    lens = torch.reshape(torch.sum(data[:p[0]+1]), (-1,1))\n",
    "    for i in range(len(s)-1):\n",
    "        # print(p[i]+1,p[i+1]+1)\n",
    "        lens = torch.cat([lens,torch.reshape(torch.sum(data[p[i]+1:p[i+1]+1]), (-1,1))])\n",
    "    max_sub_seq_cost = lens.view(-1,).max()\n",
    "    for i in range(pp-1):\n",
    "        max_sub_seq_cost += cost_c_data[p[i]][i]\n",
    "    return max_sub_seq_cost\n",
    "# partition, _ = pipe_ast(len(cost_e), np.asarray(cost_e), np.asarray(cost_c), int(pp.item()), int(B.item()))\n",
    "# trangan 只有25层，需要加一些处理\n",
    "def pipe_rl(models, L, cost_e, cost_c, k, B):\n",
    "    if k==1:\n",
    "        return [cost_e.size(0)], None\n",
    "    # print(cost_e.size(),cost_e)\n",
    "    # print(cost_c.size(),cost_c)\n",
    "    ori_data = cost_e.view(1,-1,1).cuda()\n",
    "    cost_c_data = cost_c[None,...].cuda()\n",
    "    max_c = cost_c.max()\n",
    "    count_c = 0\n",
    "    while max_c <1:\n",
    "        count_c+=1\n",
    "        max_c = max_c * 10\n",
    "    max_e = cost_e.max()\n",
    "    count_e = 0\n",
    "    while max_e <1:\n",
    "        count_e+=1\n",
    "        max_e = max_e * 10\n",
    "#     print(\"count_e: \",count_e )\n",
    "#     print(\"count_c: \",count_c )\n",
    "    \n",
    "    time1=time.time()\n",
    "    new_data = []\n",
    "    new_sample = []\n",
    "    # n_cost_e = pow(10,count_e-1) * cost_e\n",
    "    # n_cost_c = pow(10,count_c-1) * cost_c\n",
    "    n_cost_e = cost_e/cost_e.max()#pow(10,count_e-1) * cost_e\n",
    "    n_cost_c = cost_c/cost_c.max()#pow(10,count_c-1) * cost_c\n",
    "    # print(n_cost_e)\n",
    "    # print(n_cost_c)\n",
    "    # for j in range(cost_e.size(0)-1):\n",
    "    for j in range(cost_e.shape[0]-1):\n",
    "        new_sample.append([sum(n_cost_e[:j+1]),sum(n_cost_e[j+1:])]+n_cost_c[j,:].tolist())\n",
    "    new_data.append(new_sample[0])\n",
    "    # input_data =  torch.FloatTensor(new_data).cuda()\n",
    "    input_data = mindspore.Tensor(new_data,dtype=mindspore.float32)\n",
    "    model = models[k]\n",
    "    set_decode_type(model, \"greedy\")\n",
    "    \n",
    "    cost, log_likelihood, pi = model(input_data, ori_data, cost_c_data, return_pi=True)\n",
    "    # print(pi)\n",
    "    part = pi2partition(pi[0].tolist(),cost_e.size(0))\n",
    "    time2= time.time()\n",
    "    print(\"GNN cost: \", time2-time1, \"cost: \", cost)\n",
    "    return part, None\n",
    "    # gnn_cots = get_partiton_cost_sequence(ori_data.view(-1),cost_c_data[0,...],part)\n",
    "# partition, _ = pipe_ast(len(cost_e), np.asarray(cost_e), np.asarray(cost_c), int(pp.item()), int(B.item()))\n",
    "def pipe_rl_sample(models, L, cost_e, cost_c, k, B,batchsize=1024):\n",
    "    if k==1:\n",
    "        # return [cost_e.size(0)], None\n",
    "        return [cost_e.shape[0]], None\n",
    "    # print(cost_e.size(),cost_e)\n",
    "    # print(cost_c.size(),cost_c)\n",
    "    # ori_data = cost_e.view(1,-1,1).cuda()\n",
    "    ori_data = cost_e.view(1,-1,1)\n",
    "    # cost_c_data = cost_c[None,...].cuda()\n",
    "    cost_c_data = cost_c[None,...]\n",
    "    max_c = cost_c.max()\n",
    "    count_c = 0\n",
    "    while max_c <1:\n",
    "        count_c+=1\n",
    "        max_c = max_c * 10\n",
    "    max_e = cost_e.max()\n",
    "    count_e = 0\n",
    "    while max_e <1:\n",
    "        count_e+=1\n",
    "        max_e = max_e * 10\n",
    "    # print(\"count_e: \",count_e )\n",
    "    # print(\"count_c: \",count_c )\n",
    "    \n",
    "    time1=time.time()\n",
    "    new_data = []\n",
    "    new_sample = []\n",
    "    # n_cost_e = pow(10,count_e-1) * cost_e\n",
    "    # n_cost_c = pow(10,count_c-1) * cost_c\n",
    "    n_cost_e = cost_e/cost_e.max()#pow(10,count_e-1) * cost_e\n",
    "    n_cost_c = cost_c/cost_c.max()#pow(10,count_c-1) * cost_c\n",
    "    # n_cost_c = torch.ones((cost_c.size(0),cost_c.size(1))).cuda() * 0.5\n",
    "    # print(\"n_cost_e:\",n_cost_e,\"\\n\",type(n_cost_e[0]))\n",
    "    # print(\"n_cost_c:\",n_cost_c)\n",
    "    # for j in range(cost_e.size(0)-1):\n",
    "\n",
    "    for j in range(cost_e.shape[0]-1):\n",
    "        new_sample.append([sum(n_cost_e[:j+1]).asnumpy(),sum(n_cost_e[j+1:]).asnumpy()]+n_cost_c[j,:].asnumpy().tolist())\n",
    "    new_data.append(new_sample)\n",
    "    # print(\"new_data\",new_data.size())\n",
    "    # input_data =  torch.FloatTensor(new_data).cuda()\n",
    "    input_data = mindspore.Tensor(new_data,dtype=mindspore.float32)\n",
    "    model = models[k]\n",
    "    set_decode_type(model, \"greedy\")\n",
    "    \n",
    "    # cost, log_likelihood, pi = model(input_data, ori_data, cost_c_data, return_pi=True)\n",
    "    log_likelihood, pi = model(input_data, ori_data, cost_c_data, return_pi=True)\n",
    "    cost, mask = get_pp_costs(ori_data, cost_c_data, input_data, pi)\n",
    "    # print(type(pi[0]))\n",
    "    # best_partition = pi2partition(pi[0].tolist(),cost_e.size(0))\n",
    "    best_partition = pi2partition(pi[0].asnumpy().tolist(),cost_e.shape[0])\n",
    "    time2= time.time()\n",
    "    # print(\"GNN cost: \", time2-time1, \"cost: \", cost)\n",
    "    best_cost = pipe_cost(L, cost_e, cost_c, mindspore.Tensor(k), B, best_partition)\n",
    "    set_decode_type(model, \"sampling\")\n",
    "    # _, _, pis = model(input_data.repeat(batchsize,1,1), ori_data.repeat(batchsize,1,1), cost_c_data.repeat(batchsize,1,1), return_pi=True)\n",
    "    # _, _, pis = model(input_data.tile((batchsize,1,1)), ori_data.tile((batchsize,1,1)), cost_c_data.tile((batchsize,1,1)), return_pi=True)\n",
    "    _, pis = model(input_data.tile((batchsize,1,1)), ori_data.tile((batchsize,1,1)), cost_c_data.tile((batchsize,1,1)), return_pi=True)\n",
    "    # print(pis.size())\n",
    "    for i in range(pis.shape[0]):\n",
    "        # part = pi2partition(pis[i,...].tolist(),cost_e.size(0))\n",
    "        part = pi2partition(pis[i,...].tolist(),cost_e.shape[0])\n",
    "        cost = pipe_cost(L, cost_e, cost_c, mindspore.Tensor(k), B, part)\n",
    "        if cost<best_cost:\n",
    "            best_partition = part\n",
    "            best_cost=cost\n",
    "    # print(\"best cost:\", best_cost)\n",
    "    return best_partition, None\n",
    "    # gnn_cots = get_partiton_cost_sequence(ori_data.view(-1),cost_c_data[0,...],part)\n",
    "home_dir = \"/home/oj/distributed_floder/research/AMP\" #os.environ['HOME']\n",
    "workdir_path = os.path.join(home_dir, \"AMP/DeepSpeed/DeepSpeedExamples/Megatron-LM-v1.1.5-3D_parallelism\")\n",
    "example_path = os.path.join(workdir_path, \"examples\")\n",
    "sys.path.append(workdir_path)\n",
    "sys.path.append(example_path)\n",
    "\n",
    "class AMP(nn.Cell):\n",
    "    def __init__(self, model_config, exp_name, placement=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.model_config = model_config\n",
    "        self.model_type = model_config[\"type\"]\n",
    "        assert self.model_type == \"transgan\" \n",
    "        self.init_param()\n",
    "        \n",
    "        # self.model_config = model_config\n",
    "        #self.estimate = estimate\n",
    "        # self.model_type = model_config[\"type\"]\n",
    "        self.placement = placement\n",
    "        # assert self.model_type == \"gpt2\" \n",
    "        # self.init_param()\n",
    "        \n",
    "    def init_param(self):\n",
    "        h = float(self.model_config[\"hidden_size\"].item())\n",
    "        n = float(self.model_config[\"num_layers\"].item())\n",
    "        s = float(self.model_config[\"sequence_length\"].item())\n",
    "        v = float(self.model_config[\"vocab_size\"].item())\n",
    " \n",
    "        config_h = int((self.model_config[\"hidden_size\"]).item())\n",
    "        config_n = int(n)\n",
    "\n",
    "        json_path = os.path.join(example_path, \"ds_config.json\")\n",
    "\n",
    "        self.profile_cost = {}\n",
    "        for mp_size in [1,2,4]:\n",
    "            # known_cost directory stores the real forward time with correponding model parallel degree.\n",
    "            # known_record = f\"/home/oj/distributed_floder/research/AMP/src/known_cost/{self.model_type}_P3_{mp_size}\"\n",
    "            known_record = f\"/root/cym/AMP/src/known_cost/{self.model_type}_P3_{mp_size}\"\n",
    "            # We model backward time as forward time * 2\n",
    "            cur_profile_cost = 3 * np.load(f\"{known_record}.npy\")\n",
    "            self.profile_cost[str(mp_size)] = cur_profile_cost\n",
    "            print(f\"using exec cost with mp_size {mp_size}: {cur_profile_cost}\")\n",
    "        self.models = load_all_model()\n",
    "            \n",
    "    def predict(self, config, bs, mbs, cluster_info, model_config, amp_config, oth):\n",
    "        L = model_config[\"num_layers\"]\n",
    "        cost = ops.Zeros()(1,mindspore.float32)\n",
    "        M, N = config.shape\n",
    "        config = np.asarray(config)\n",
    "\n",
    "        if np.all(config == -1):\n",
    "            rank_map = defaultdict(list)\n",
    "            rank_node_map = dict()\n",
    "\n",
    "            m = oth[\"mp_deg\"]\n",
    "            n = oth[\"dp_deg\"]\n",
    "            pp = oth[\"pp_deg\"]                   \n",
    "\n",
    "            # infer a GPU rank map                \n",
    "            counter = 0    \n",
    "            for j in range(N):\n",
    "                for k in range(M):\n",
    "                    # TODO: bad code here, config counts from 1\n",
    "                    rank_map[j].append(counter)\n",
    "                    rank_node_map[counter] = j\n",
    "                    counter += 1\n",
    "\n",
    "            #print(f\"AMP estimate default to {rank_map}\")\n",
    "\n",
    "        # valid config, inferred from sa \n",
    "        else:\n",
    "            config = mindspore.Tensor.from_numpy(config)\n",
    "            # pp = torch.max(config).float()\n",
    "            pp = ops.ArgMaxWithValue()(config)[1].float()\n",
    "            # infer rank_map: given node name, returns the global mapped rank(int) in (pp, dp, mp) order\n",
    "            # rank_node_map: given rank, returns the node\n",
    "            rank_map = defaultdict(list)\n",
    "            rank_node_map = dict()\n",
    "\n",
    "            if pp >= (L + 2):\n",
    "                print(f\"early return with pp={pp}, L={L}\")\n",
    "                return None, None, torch.tensor([float(\"inf\")])\n",
    "\n",
    "            m = oth[\"mp_deg\"]\n",
    "            n = oth[\"dp_deg\"]\n",
    "            assert pp == oth[\"pp_deg\"]                   \n",
    "\n",
    "            rank_counter = np.zeros(int(pp.item()))\n",
    "\n",
    "            # infer a GPU rank map                    \n",
    "            for j in range(N):\n",
    "                for k in range(M):\n",
    "                    # TODO: bad code here, config counts from 1\n",
    "                    cur_pp = int(config[k][j] - 1)\n",
    "                    rank_map[j].append(int((rank_counter[cur_pp] + cur_pp * m * n).item()))\n",
    "                    rank_node_map[int((rank_counter[cur_pp] + cur_pp * m * n).item())] = j\n",
    "                    rank_counter[cur_pp] += 1\n",
    "\n",
    "        # infer number of micro-batch size B\n",
    "        B = bs / (n * mbs)\n",
    "\n",
    "        parallel_config = {\"mp\" : m, \"dp\" : n, \"pp\" : pp, \"micro_bs\" : mbs, \"rank_map\" : rank_map, \"rank_node_map\": rank_node_map}\n",
    "        cost_e = get_cost_e(cluster_info=cluster_info, \n",
    "                            model_config=model_config, parallel_config=parallel_config, amp_config=amp_config)\n",
    "        cost_c = get_cost_c(cluster_info=cluster_info, \n",
    "                            model_config=model_config, parallel_config=parallel_config, amp_config=amp_config)\n",
    "\n",
    "        #partition, _ = pipe_dp(int(L.item()), np.asarray(cost_e.detach()), np.asarray(cost_c.detach()), int(pp.item()), int(B.item()))\n",
    "        if int(B.item()) == 1:\n",
    "            partition, _ = pipe_uniform(int(L.item()), int(pp.item()))\n",
    "        else:\n",
    "            # partition, _ = pipe_ast(len(cost_e), np.asarray(cost_e), np.asarray(cost_c), int(pp.item()), int(B.item()))\n",
    "            print(cost_e.shape[0],cost_c.shape[0])\n",
    "            partition, _ = pipe_rl_sample(self.models, len(cost_e), cost_e, cost_c, int(pp.item()), int(B.item()))\n",
    "            # partition, _ = pipe_rl(self.models, len(cost_e), cost_e, cost_c, int(pp.item()), int(B.item()))\n",
    "        print(f\"amp gives partition: {partition}\")\n",
    "        cost = pipe_cost(L, cost_e, cost_c, pp, B, partition)\n",
    "\n",
    "        # translate to ds form, add data parallelism cost\n",
    "        ds_partition, dp_side_cost = dp_cost(config, cluster_info=cluster_info, \n",
    "                            model_config=model_config, parallel_config=parallel_config, \n",
    "                            amp_config=amp_config, partition=partition)\n",
    "\n",
    "        cost += dp_side_cost\n",
    "        #print(ds_partition, cost, dp_side_cost)\n",
    "        return rank_map, ds_partition, cost\n",
    "    def construct(self, args):\n",
    "        model_type = self.model_type\n",
    "        config, bs, micro_bs, cluster_info, model_config, oth = args\n",
    "        amp_config = {\"profile_cost\" : self.profile_cost}\n",
    "        rank_map, partition, amp_pred = self.predict(config, bs, micro_bs, cluster_info, model_config, amp_config, oth)\n",
    "        \n",
    "        return rank_map, partition, amp_pred\n",
    "global_bs = 64\n",
    "model = AMP(model_config, exp_name)\n",
    "assert (global_bs % M == 0) and (global_bs % N == 0), \"global batch size is too irrgular\"\n",
    "\n",
    "want_simulate = [] \n",
    "feasible = {}\n",
    "\n",
    "with open(record_file, \"a\") as fp:\n",
    "    fp.write(f\"{model_config}\\n\")                \n",
    "    fp.write(f\"gbs:{global_bs}\\n\")                \n",
    "known = None\n",
    "iter_count = 0\n",
    "time_s = time.time()\n",
    "print(\"1\")\n",
    "# Estimating best configurations\n",
    "while True:\n",
    "    ret = amp_no_placement_strategy(M=M, N=N, gbs=global_bs, known=known)\n",
    "    if ret is None:\n",
    "        break\n",
    "    else:\n",
    "\n",
    "        h, w, mbs, known = ret\n",
    "        # oth = {\"mp_deg\": torch.ones(1,)*h, \"dp_deg\": torch.ones(1,)*w, \"pp_deg\": torch.ones(1,)*(M*N/(h*w))}\n",
    "        oth = {\"mp_deg\": ops.ones(1,mindspore.float32)*h, \"dp_deg\": ops.ones(1,mindspore.float32)*w, \"pp_deg\": ops.ones(1,mindspore.float32)*(M*N/(h*w))}\n",
    "        fake_config = np.ones((M,N)) * (-1) \n",
    "        model_args = (fake_config, global_bs, mbs, cluster_info, model_config, oth)    \n",
    "        \n",
    "        # print(model(model_args))\n",
    "        \n",
    "        rank_map, partition, cost = model(model_args)\n",
    "        \n",
    "\n",
    "        want_simulate.append(((mbs, oth, rank_map, partition), cost))\n",
    "    iter_count += 1\n",
    "    if iter_count % 10 == 0:\n",
    "        print(f\"AMP finish {iter_count} iterations\")\n",
    "        \n",
    "\n",
    "time_e = time.time()\n",
    "print(f\"AMP finishes without placement in {iter_count} iterations in {time_e - time_s}\")\n",
    "\n",
    "\n",
    "sorted_settings = sorted(want_simulate, key = lambda kv: kv[1])\n",
    "with open(record_file, \"a\") as fp:\n",
    "    for item in sorted_settings:\n",
    "        fp.write(f\"rank {sorted_settings.index(item)}: {item}\")\n",
    "        fp.write(\"\\n\")\n",
    "\n",
    "time_e-time_s\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
