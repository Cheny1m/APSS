{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 99, cpu_mem 2427.96875MB -> 2428.21875MB\n",
      "Step 199, cpu_mem 2427.96875MB -> 2428.30078125MB\n",
      "Step 299, cpu_mem 2427.96875MB -> 2428.7890625MB\n",
      "Step 399, cpu_mem 2427.96875MB -> 2429.05078125MB\n",
      "Step 499, cpu_mem 2427.96875MB -> 2429.31640625MB\n",
      "Step 599, cpu_mem 2427.96875MB -> 2429.3203125MB\n",
      "Step 699, cpu_mem 2427.96875MB -> 2429.84375MB\n",
      "Step 799, cpu_mem 2427.96875MB -> 2430.10546875MB\n",
      "Step 899, cpu_mem 2427.96875MB -> 2430.3671875MB\n",
      "Step 999, cpu_mem 2427.96875MB -> 2430.62890625MB\n",
      "Step 1099, cpu_mem 2427.96875MB -> 2430.890625MB\n",
      "Step 1199, cpu_mem 2427.96875MB -> 2431.1484375MB\n",
      "Step 1299, cpu_mem 2427.96875MB -> 2432.73828125MB\n",
      "Step 1399, cpu_mem 2427.96875MB -> 2434.80078125MB\n",
      "Step 1499, cpu_mem 2427.96875MB -> 2437.12109375MB\n",
      "Step 1599, cpu_mem 2427.96875MB -> 2439.69921875MB\n",
      "Step 1699, cpu_mem 2427.96875MB -> 2442.01953125MB\n",
      "Step 1799, cpu_mem 2427.96875MB -> 2444.33984375MB\n",
      "Step 1899, cpu_mem 2427.96875MB -> 2446.14453125MB\n",
      "Step 1999, cpu_mem 2427.96875MB -> 2448.72265625MB\n",
      "All done: cpu_mem 2427.96875MB -> 2448.72265625MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psutil\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "import mindspore as ms\n",
    "from mindspore import nn, Tensor, ops\n",
    "\n",
    "# 查询当前进程的内存，单位为MB\n",
    "def get_cpu_memory(pid=os.getpid()):\n",
    "    p = psutil.Process(pid)\n",
    "    return p.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "class TestNet(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(TestNet, self).__init__()\n",
    "        self._multinomial = ops.Multinomial()\n",
    "\n",
    "    def construct(self, x):\n",
    "        # 使用ops.multinomial函数式调用，存在泄露\n",
    "        action = ops.multinomial(x, 1)\n",
    "        # action = self._multinomial(x, 1)\n",
    "        return action\n",
    "\n",
    "def test_mem_leak():\n",
    "    bs = 128\n",
    "    c = 1024\n",
    "    net = TestNet()\n",
    "    x = Tensor(np.random.rand(bs, c), ms.float32)\n",
    "    # warmup\n",
    "    for i in range(50):\n",
    "        y = net(x)\n",
    "        y_arr = y.asnumpy()\n",
    "\n",
    "    mem0 = get_cpu_memory()\n",
    "    mem1 = 0\n",
    "    for i in range(2000):\n",
    "        y = net(x)\n",
    "        y_arr = y.asnumpy()\n",
    "        del y\n",
    "        del y_arr\n",
    "        mem1 = get_cpu_memory()     \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Step {}, cpu_mem {}MB -> {}MB'.format(i, mem0, mem1))\n",
    "\n",
    "    print('All done: cpu_mem {}MB -> {}MB'.format(mem0, mem1))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ms.context.set_context(device_target='GPU', mode=ms.context.PYNATIVE_MODE)\n",
    "    # ms.context.set_context(device_target='GPU', mode=ms.context.GRAPH_MODE)\n",
    "\n",
    "    test_mem_leak()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tensor(shape=[], dtype=Int64, value= 3)]\n",
      "[Tensor(shape=[], dtype=Int64, value= 4)]\n",
      "[Tensor(shape=[], dtype=Int64, value= 5)]\n",
      "[Tensor(shape=[], dtype=Int64, value= 3)]\n",
      "[Tensor(shape=[], dtype=Int64, value= 4)]\n",
      "[Tensor(shape=[], dtype=Int64, value= 5)]\n",
      "[Tensor(shape=[], dtype=Int64, value= 3)]\n",
      "[Tensor(shape=[], dtype=Int64, value= 4)]\n",
      "[Tensor(shape=[], dtype=Int64, value= 5)]\n",
      "[Tensor(shape=[], dtype=Int64, value= 3)]\n",
      "[Tensor(shape=[], dtype=Int64, value= 4)]\n",
      "[Tensor(shape=[], dtype=Int64, value= 5)]\n",
      "[Tensor(shape=[], dtype=Int64, value= 3)]\n",
      "[Tensor(shape=[], dtype=Int64, value= 4)]\n",
      "[Tensor(shape=[], dtype=Int64, value= 5)]\n"
     ]
    }
   ],
   "source": [
    "from mindspore.dataset import MnistDataset, GeneratorDataset\n",
    "# # since a generator instance can be only iterated once, we need to wrap it by lambda to generate multiple instances\n",
    "dataset = GeneratorDataset(source=lambda: my_generator(3, 6), column_names=[\"data\"])\n",
    "\n",
    "# for d in dataset:\n",
    "#     print(d)\n",
    "\n",
    "# My_generator = my_generator(3, 6)\n",
    "# dataset = GeneratorDataset(source=My_generator, column_names=[\"data\"])\n",
    "\n",
    "for i in range(5):\n",
    "    for d in dataset:\n",
    "        print(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> {'column1': Tensor(shape=[2], dtype=Int64, value= [6, 8])} <class 'mindspore.common.tensor.Tensor'> 8\n"
     ]
    }
   ],
   "source": [
    "import mindspore.dataset as ds\n",
    "dataset = ds.GeneratorDataset([i for i in range(10)], \"column1\").batch(batch_size=2)\n",
    "iterator = dataset.create_dict_iterator()\n",
    "for item in iterator:\n",
    "    # item is a list\n",
    "    print(type(item),item,type(item[\"column1\"]),item[\"column1\"][1])\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "class Network(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense_relu_sequential = nn.SequentialCell(\n",
    "            nn.Dense(28*28, 512, weight_init=\"normal\", bias_init=\"zeros\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(512, 512, weight_init=\"normal\", bias_init=\"zeros\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(512, 10, weight_init=\"normal\", bias_init=\"zeros\")\n",
    "        )\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.dense_relu_sequential(x)\n",
    "        return logits\n",
    "\n",
    "model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: Network<\n",
      "  (flatten): Flatten<>\n",
      "  (dense_relu_sequential): SequentialCell<\n",
      "    (0): Dense<input_channels=784, output_channels=512, has_bias=True>\n",
      "    (1): ReLU<>\n",
      "    (2): Dense<input_channels=512, output_channels=512, has_bias=True>\n",
      "    (3): ReLU<>\n",
      "    (4): Dense<input_channels=512, output_channels=10, has_bias=True>\n",
      "    >\n",
      "  >\n",
      "\n",
      "\n",
      "Layer: dense_relu_sequential.0.weight\n",
      "Size: (512, 784)\n",
      "Values : Parameter (name=dense_relu_sequential.0.weight, shape=(512, 784), dtype=Float32, requires_grad=True) \n",
      "\n",
      "Layer: dense_relu_sequential.0.bias\n",
      "Size: (512,)\n",
      "Values : Parameter (name=dense_relu_sequential.0.bias, shape=(512,), dtype=Float32, requires_grad=True) \n",
      "\n",
      "Layer: dense_relu_sequential.2.weight\n",
      "Size: (512, 512)\n",
      "Values : Parameter (name=dense_relu_sequential.2.weight, shape=(512, 512), dtype=Float32, requires_grad=True) \n",
      "\n",
      "Layer: dense_relu_sequential.2.bias\n",
      "Size: (512,)\n",
      "Values : Parameter (name=dense_relu_sequential.2.bias, shape=(512,), dtype=Float32, requires_grad=True) \n",
      "\n",
      "Layer: dense_relu_sequential.4.weight\n",
      "Size: (10, 512)\n",
      "Values : Parameter (name=dense_relu_sequential.4.weight, shape=(10, 512), dtype=Float32, requires_grad=True) \n",
      "\n",
      "Layer: dense_relu_sequential.4.bias\n",
      "Size: (10,)\n",
      "Values : Parameter (name=dense_relu_sequential.4.bias, shape=(10,), dtype=Float32, requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.parameters_and_names():\n",
    "    print(f\"Layer: {name}\\nSize: {param.shape}\\nValues : {param} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward inputs:  (Tensor(shape=[1], dtype=Float32, value= [ 3.00000000e+00]),)\n",
      "[3.]\n",
      "forward inputs:  (Tensor(shape=[1], dtype=Float32, value= [ 3.00000000e+00]),)\n",
      "(Tensor(shape=[1], dtype=Float32, value= [ 2.00000000e+00]), Tensor(shape=[1], dtype=Float32, value= [ 1.00000000e+00]))\n",
      "(Tensor(shape=[1], dtype=Float32, value= [ 2.00000000e+00]), Tensor(shape=[1], dtype=Float32, value= [ 1.00000000e+00]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "\n",
    "ms.set_context(mode=ms.PYNATIVE_MODE)\n",
    "\n",
    "def forward_pre_hook_fn(cell_id, inputs):\n",
    "    print(\"forward inputs: \", inputs)\n",
    "    return inputs\n",
    "\n",
    "class Net(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.handle = self.relu.register_forward_pre_hook(forward_pre_hook_fn)\n",
    "\n",
    "    def construct(self, x, y):\n",
    "        x = 2 * x + y\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "grad_net = ms.grad(net, grad_position=(0, 1))\n",
    "\n",
    "x = ms.Tensor(np.ones([1]).astype(np.float32))\n",
    "y = ms.Tensor(np.ones([1]).astype(np.float32))\n",
    "\n",
    "output = net(x, y)\n",
    "print(output)\n",
    "gradient = grad_net(x, y)\n",
    "print(gradient)\n",
    "net.handle.remove()\n",
    "gradient = grad_net(x, y)\n",
    "print(gradient)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
